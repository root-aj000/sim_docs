OK. Here's a comprehensive breakdown of the provided TypeScript code, designed for clarity and understanding.

**Purpose of this file:**

This TypeScript file defines a Zustand store called `useKnowledgeStore`. This store is responsible for managing the state related to knowledge bases, documents, and chunks within an application. It handles fetching, caching, updating, and deleting these entities. It also manages the loading state for each of these operations. This store uses an in-memory approach for caching and avoids redundant API calls.

**Key Concepts:**

*   **Zustand:**  A small, fast, and scalable bearbones state-management solution for React applications.  It's used here to create a centralized store that components can subscribe to.
*   **Knowledge Base:** A collection of documents.
*   **Document:** A single file or piece of content within a knowledge base.
*   **Chunk:**  A smaller segment of a document, typically created for efficient embedding and retrieval in a vector database.
*   **Caching:** Storing fetched data in memory to avoid redundant API calls, improving performance. The implementation caches knowledge bases, documents, and chunks.
*   **Loading States:**  Tracking whether data is currently being fetched, preventing multiple concurrent requests and providing visual feedback to the user.
*   **Immutability:**  The store updates its state using immutable updates (creating new objects instead of modifying existing ones), which is essential for predictable state management in React.
*   **Asynchronous Operations:** The store handles asynchronous operations (fetching data from APIs) using `async/await`.

**Code Explanation (Line by Line):**

```typescript
import { create } from 'zustand'
import { createLogger } from '@/lib/logs/console/logger'
```

*   `import { create } from 'zustand'`: Imports the `create` function from the `zustand` library. This function is used to create a Zustand store.
*   `import { createLogger } from '@/lib/logs/console/logger'`: Imports a `createLogger` function from a local file. This is likely used for logging actions and state changes within the store.

```typescript
const logger = createLogger('KnowledgeStore')
```

*   `const logger = createLogger('KnowledgeStore')`: Creates a logger instance specifically for this store, labeling the logs with "KnowledgeStore". This makes it easier to filter and identify logs related to this store.

```typescript
export interface ChunkingConfig {
  maxSize: number
  minSize: number
  overlap: number
  chunkSize?: number // Legacy support
  minCharactersPerChunk?: number // Legacy support
  recipe?: string
  lang?: string
  strategy?: 'recursive' | 'semantic' | 'sentence' | 'paragraph'
  [key: string]: unknown
}
```

*   `export interface ChunkingConfig`: Defines an interface for the configuration settings used when splitting documents into smaller chunks.
    *   `maxSize`: The maximum size of a chunk (likely in tokens or characters).
    *   `minSize`: The minimum size of a chunk.
    *   `overlap`: The amount of overlap between consecutive chunks. This helps maintain context between chunks.
    *   `chunkSize?: number`: (Optional) Legacy support for a chunk size setting.
    *   `minCharactersPerChunk?: number`: (Optional) Legacy support for a minimum characters per chunk setting.
    *   `recipe?: string`: (Optional) Specifies the recipe or method to use for chunking.
    *   `lang?: string`: (Optional) Specifies the language of the text being chunked.
    *   `strategy?: 'recursive' | 'semantic' | 'sentence' | 'paragraph'`: (Optional) Specifies the splitting strategy:
        *   `recursive`: Split recursively until chunks meet the size criteria.
        *   `semantic`: Split based on semantic meaning.
        *   `sentence`: Split by sentence boundaries.
        *   `paragraph`: Split by paragraph boundaries.
    *   `[key: string]: unknown`: Allows for arbitrary additional properties in the `ChunkingConfig` object, providing flexibility for future configuration options.

```typescript
export interface KnowledgeBaseData {
  id: string
  name: string
  description?: string
  tokenCount: number
  embeddingModel: string
  embeddingDimension: number
  chunkingConfig: ChunkingConfig
  createdAt: string
  updatedAt: string
  workspaceId?: string
}
```

*   `export interface KnowledgeBaseData`: Defines an interface representing a knowledge base.
    *   `id`: Unique identifier of the knowledge base.
    *   `name`: Name of the knowledge base.
    *   `description?: string`: (Optional) Description of the knowledge base.
    *   `tokenCount`: The total number of tokens in the knowledge base.
    *   `embeddingModel`: The name of the embedding model used for this knowledge base (e.g., "OpenAI ada-002").
    *   `embeddingDimension`: The dimension of the embeddings generated by the embedding model.
    *   `chunkingConfig`: The `ChunkingConfig` used for documents in this knowledge base.
    *   `createdAt`: Timestamp indicating when the knowledge base was created.
    *   `updatedAt`: Timestamp indicating when the knowledge base was last updated.
    *   `workspaceId?: string`: (Optional)  The ID of the workspace this knowledge base belongs to.

```typescript
export interface DocumentData {
  id: string
  knowledgeBaseId: string
  filename: string
  fileUrl: string
  fileSize: number
  mimeType: string
  chunkCount: number
  tokenCount: number
  characterCount: number
  processingStatus: 'pending' | 'processing' | 'completed' | 'failed'
  processingStartedAt?: string | null
  processingCompletedAt?: string | null
  processingError?: string | null
  enabled: boolean
  uploadedAt: string
  // Document tags
  tag1?: string | null
  tag2?: string | null
  tag3?: string | null
  tag4?: string | null
  tag5?: string | null
  tag6?: string | null
  tag7?: string | null
}
```

*   `export interface DocumentData`: Defines an interface representing a document.
    *   `id`: Unique identifier of the document.
    *   `knowledgeBaseId`: The ID of the knowledge base this document belongs to.
    *   `filename`: The name of the file.
    *   `fileUrl`: The URL where the file can be accessed.
    *   `fileSize`: The size of the file in bytes.
    *   `mimeType`: The MIME type of the file (e.g., "application/pdf", "text/plain").
    *   `chunkCount`: The number of chunks this document has been split into.
    *   `tokenCount`: The total number of tokens in the document.
    *   `characterCount`: The total number of characters in the document.
    *   `processingStatus`: The current processing status of the document.  It can be:
        *   `'pending'`: Waiting to be processed.
        *   `'processing'`: Currently being processed.
        *   `'completed'`: Processing is finished.
        *   `'failed'`: Processing failed.
    *   `processingStartedAt?: string | null`: (Optional) Timestamp indicating when processing started.
    *   `processingCompletedAt?: string | null`: (Optional) Timestamp indicating when processing completed.
    *   `processingError?: string | null`: (Optional) Error message if processing failed.
    *   `enabled`: A boolean indicating whether the document is enabled or not.
    *   `uploadedAt`: Timestamp indicating when the document was uploaded.
    *   `tag1` - `tag7`:  Optional tags associated with the document.

```typescript
export interface ChunkData {
  id: string
  chunkIndex: number
  content: string
  contentLength: number
  tokenCount: number
  enabled: boolean
  startOffset: number
  endOffset: number
  tag1?: string | null
  tag2?: string | null
  tag3?: string | null
  tag4?: string | null
  tag5?: string | null
  tag6?: string | null
  tag7?: string | null
  createdAt: string
  updatedAt: string
}
```

*   `export interface ChunkData`: Defines an interface representing a chunk of a document.
    *   `id`: Unique identifier of the chunk.
    *   `chunkIndex`: The index of this chunk within the document (0-based).
    *   `content`: The actual text content of the chunk.
    *   `contentLength`: The length of the content string (in characters).
    *   `tokenCount`: The number of tokens in the chunk.
    *   `enabled`: A boolean indicating whether the chunk is enabled or not.
    *   `startOffset`: The starting character offset of this chunk within the original document.
    *   `endOffset`: The ending character offset of this chunk within the original document.
    *   `tag1` - `tag7`: Optional tags associated with the chunk.
    *   `createdAt`: Timestamp indicating when the chunk was created.
    *   `updatedAt`: Timestamp indicating when the chunk was last updated.

```typescript
export interface ChunksPagination {
  total: number
  limit: number
  offset: number
  hasMore: boolean
}

export interface DocumentsPagination {
  total: number
  limit: number
  offset: number
  hasMore: boolean
}
```

*   `export interface ChunksPagination`: Defines an interface for pagination information related to chunks.
    *   `total`: The total number of chunks available.
    *   `limit`: The maximum number of chunks returned per page.
    *   `offset`: The starting index of the current page.
    *   `hasMore`: A boolean indicating whether there are more chunks available beyond the current page.
*   `export interface DocumentsPagination`: Defines an interface for pagination information related to documents.
    *   `total`: The total number of documents available.
    *   `limit`: The maximum number of documents returned per page.
    *   `offset`: The starting index of the current page.
    *   `hasMore`: A boolean indicating whether there are more documents available beyond the current page.

```typescript
export interface ChunksCache {
  chunks: ChunkData[]
  pagination: ChunksPagination
  searchQuery?: string
  lastFetchTime: number
}

export interface DocumentsCache {
  documents: DocumentData[]
  pagination: DocumentsPagination
  searchQuery?: string
  sortBy?: string
  sortOrder?: string
  lastFetchTime: number
}
```

*   `export interface ChunksCache`: Defines an interface for caching chunks.
    *   `chunks`: An array of `ChunkData` representing the cached chunks.
    *   `pagination`: The `ChunksPagination` information for the cached chunks.
    *   `searchQuery?: string`: (Optional) The search query used to fetch these chunks. This is used to determine if the cache is valid for a new search.
    *   `lastFetchTime`: A timestamp indicating when the chunks were last fetched.
*   `export interface DocumentsCache`: Defines an interface for caching documents.
    *   `documents`: An array of `DocumentData` representing the cached documents.
    *   `pagination`: The `DocumentsPagination` information for the cached documents.
    *   `searchQuery?: string`: (Optional) The search query used to fetch these documents.
    *   `sortBy?: string`: (Optional) The field used to sort the documents.
    *   `sortOrder?: string`: (Optional) The sort order ("asc" or "desc").
    *   `lastFetchTime`: A timestamp indicating when the documents were last fetched.

```typescript
interface KnowledgeStore {
  // State
  knowledgeBases: Record<string, KnowledgeBaseData>
  documents: Record<string, DocumentsCache> // knowledgeBaseId -> documents cache
  chunks: Record<string, ChunksCache> // documentId -> chunks cache
  knowledgeBasesList: KnowledgeBaseData[]

  // Loading states
  loadingKnowledgeBases: Set<string>
  loadingDocuments: Set<string>
  loadingChunks: Set<string>
  loadingKnowledgeBasesList: boolean
  knowledgeBasesListLoaded: boolean

  // Actions
  getKnowledgeBase: (id: string) => Promise<KnowledgeBaseData | null>
  getDocuments: (
    knowledgeBaseId: string,
    options?: {
      search?: string
      limit?: number
      offset?: number
      sortBy?: string
      sortOrder?: string
    }
  ) => Promise<DocumentData[]>
  getChunks: (
    knowledgeBaseId: string,
    documentId: string,
    options?: { search?: string; limit?: number; offset?: number }
  ) => Promise<ChunkData[]>
  getKnowledgeBasesList: (workspaceId?: string) => Promise<KnowledgeBaseData[]>
  refreshDocuments: (
    knowledgeBaseId: string,
    options?: {
      search?: string
      limit?: number
      offset?: number
      sortBy?: string
      sortOrder?: string
    }
  ) => Promise<DocumentData[]>
  refreshChunks: (
    knowledgeBaseId: string,
    documentId: string,
    options?: { search?: string; limit?: number; offset?: number }
  ) => Promise<ChunkData[]>
  updateDocument: (
    knowledgeBaseId: string,
    documentId: string,
    updates: Partial<DocumentData>
  ) => void
  updateChunk: (documentId: string, chunkId: string, updates: Partial<ChunkData>) => void
  addPendingDocuments: (knowledgeBaseId: string, documents: DocumentData[]) => void
  addKnowledgeBase: (knowledgeBase: KnowledgeBaseData) => void
  updateKnowledgeBase: (id: string, updates: Partial<KnowledgeBaseData>) => void
  removeKnowledgeBase: (id: string) => void
  removeDocument: (knowledgeBaseId: string, documentId: string) => void
  clearDocuments: (knowledgeBaseId: string) => void
  clearChunks: (documentId: string) => void
  clearKnowledgeBasesList: () => void

  // Getters
  getCachedKnowledgeBase: (id: string) => KnowledgeBaseData | null
  getCachedDocuments: (knowledgeBaseId: string) => DocumentsCache | null
  getCachedChunks: (documentId: string, options?: { search?: string }) => ChunksCache | null

  // Loading state getters
  isKnowledgeBaseLoading: (id: string) => boolean
  isDocumentsLoading: (knowledgeBaseId: string) => boolean
  isChunksLoading: (documentId: string) => boolean
}
```

*   `interface KnowledgeStore`: Defines the overall structure of the Zustand store.
    *   **State:**
        *   `knowledgeBases`: A record (object) where keys are knowledge base IDs (strings) and values are `KnowledgeBaseData` objects.  This stores the knowledge bases themselves.
        *   `documents`: A record where keys are knowledge base IDs and values are `DocumentsCache` objects.  This caches the documents for each knowledge base.
        *   `chunks`: A record where keys are document IDs and values are `ChunksCache` objects. This caches the chunks for each document.
        *   `knowledgeBasesList`: An array containing a list of `KnowledgeBaseData` objects.
    *   **Loading States:**
        *   `loadingKnowledgeBases`: A `Set` containing the IDs of knowledge bases that are currently being loaded.  Using a `Set` allows for efficient checking if a specific knowledge base is loading.
        *   `loadingDocuments`: A `Set` containing the IDs of knowledge bases for which documents are currently being loaded.
        *   `loadingChunks`: A `Set` containing the IDs of documents for which chunks are currently being loaded.
        *   `loadingKnowledgeBasesList`: A boolean indicating whether the knowledge base list is currently being loaded.
        *   `knowledgeBasesListLoaded`: A boolean indicating whether the knowledge bases list has been loaded at least once. This is important to prevent infinite loops if the list is empty or errors out on initial load.
    *   **Actions:** These are functions that modify the state of the store.  They typically involve fetching data from an API, updating the cache, and updating the loading state.
        *   `getKnowledgeBase`: Fetches a knowledge base by its ID.
        *   `getDocuments`: Fetches documents for a given knowledge base, with optional search, pagination, and sorting.
        *   `getChunks`: Fetches chunks for a given document, with optional search and pagination.
        *   `getKnowledgeBasesList`: Fetches a list of knowledge bases, optionally filtered by workspace ID.
        *   `refreshDocuments`: Refreshes the cached documents for a knowledge base.
        *   `refreshChunks`: Refreshes the cached chunks for a document.
        *   `updateDocument`: Updates a specific document.
        *   `updateChunk`: Updates a specific chunk.
        *   `addPendingDocuments`: Adds new documents to a knowledge base's document list.
        *   `addKnowledgeBase`: Adds a new knowledge base to the store.
        *   `updateKnowledgeBase`: Updates a knowledge base.
        *   `removeKnowledgeBase`: Removes a knowledge base.
        *   `removeDocument`: Removes a document from a knowledge base.
        *   `clearDocuments`: Clears the cached documents for a knowledge base.
        *   `clearChunks`: Clears the cached chunks for a document.
        *   `clearKnowledgeBasesList`: Clears the cached knowledge bases list and resets the loaded state.
    *   **Getters:** These are functions that retrieve data from the store without modifying the state.
        *   `getCachedKnowledgeBase`: Retrieves a knowledge base from the cache.
        *   `getCachedDocuments`: Retrieves the cached documents for a knowledge base.
        *   `getCachedChunks`: Retrieves the cached chunks for a document.
    *   **Loading State Getters:**
        *   `isKnowledgeBaseLoading`: Checks if a knowledge base is currently loading.
        *   `isDocumentsLoading`: Checks if documents are currently loading for a knowledge base.
        *   `isChunksLoading`: Checks if chunks are currently loading for a document.

```typescript
export const useKnowledgeStore = create<KnowledgeStore>((set, get) => ({
  knowledgeBases: {},
  documents: {},
  chunks: {},
  knowledgeBasesList: [],
  loadingKnowledgeBases: new Set(),
  loadingDocuments: new Set(),
  loadingChunks: new Set(),
  loadingKnowledgeBasesList: false,
  knowledgeBasesListLoaded: false,

  getCachedKnowledgeBase: (id: string) => {
    return get().knowledgeBases[id] || null
  },

  getCachedDocuments: (knowledgeBaseId: string) => {
    return get().documents[knowledgeBaseId] || null
  },

  getCachedChunks: (documentId: string, options?: { search?: string }) => {
    return get().chunks[documentId] || null
  },

  isKnowledgeBaseLoading: (id: string) => {
    return get().loadingKnowledgeBases.has(id)
  },

  isDocumentsLoading: (knowledgeBaseId: string) => {
    return get().loadingDocuments.has(knowledgeBaseId)
  },

  isChunksLoading: (documentId: string) => {
    return get().loadingChunks.has(documentId)
  },

  getKnowledgeBase: async (id: string) => {
    const state = get()

    // Return cached data if it exists
    const cached = state.knowledgeBases[id]
    if (cached) {
      return cached
    }

    // Return cached data if already loading to prevent duplicate requests
    if (state.loadingKnowledgeBases.has(id)) {
      return null
    }

    try {
      set((state) => ({
        loadingKnowledgeBases: new Set([...state.loadingKnowledgeBases, id]),
      }))

      const response = await fetch(`/api/knowledge/${id}`)

      if (!response.ok) {
        throw new Error(`Failed to fetch knowledge base: ${response.statusText}`)
      }

      const result = await response.json()

      if (!result.success) {
        throw new Error(result.error || 'Failed to fetch knowledge base')
      }

      const knowledgeBase = result.data

      set((state) => ({
        knowledgeBases: {
          ...state.knowledgeBases,
          [id]: knowledgeBase,
        },
        loadingKnowledgeBases: new Set(
          [...state.loadingKnowledgeBases].filter((loadingId) => loadingId !== id)
        ),
      }))

      logger.info(`Knowledge base loaded: ${id}`)
      return knowledgeBase
    } catch (error) {
      logger.error(`Error fetching knowledge base ${id}:`, error)

      set((state) => ({
        loadingKnowledgeBases: new Set(
          [...state.loadingKnowledgeBases].filter((loadingId) => loadingId !== id)
        ),
      }))

      throw error
    }
  },

  getDocuments: async (
    knowledgeBaseId: string,
    options?: {
      search?: string
      limit?: number
      offset?: number
      sortBy?: string
      sortOrder?: string
    }
  ) => {
    const state = get()

    // Check if we have cached data that matches the exact request parameters
    const cached = state.documents[knowledgeBaseId]
    const requestLimit = options?.limit || 50
    const requestOffset = options?.offset || 0
    const requestSearch = options?.search
    const requestSortBy = options?.sortBy
    const requestSortOrder = options?.sortOrder

    if (
      cached &&
      cached.searchQuery === requestSearch &&
      cached.pagination.limit === requestLimit &&
      cached.pagination.offset === requestOffset &&
      cached.sortBy === requestSortBy &&
      cached.sortOrder === requestSortOrder
    ) {
      return cached.documents
    }

    // Return empty array if already loading to prevent duplicate requests
    if (state.loadingDocuments.has(knowledgeBaseId)) {
      return cached?.documents || []
    }

    try {
      set((state) => ({
        loadingDocuments: new Set([...state.loadingDocuments, knowledgeBaseId]),
      }))

      // Build query parameters using the same defaults as caching
      const params = new URLSearchParams()
      if (requestSearch) params.set('search', requestSearch)
      if (requestSortBy) params.set('sortBy', requestSortBy)
      if (requestSortOrder) params.set('sortOrder', requestSortOrder)
      params.set('limit', requestLimit.toString())
      params.set('offset', requestOffset.toString())

      const url = `/api/knowledge/${knowledgeBaseId}/documents${params.toString() ? `?${params.toString()}` : ''}`
      const response = await fetch(url)

      if (!response.ok) {
        throw new Error(`Failed to fetch documents: ${response.statusText}`)
      }

      const result = await response.json()

      if (!result.success) {
        throw new Error(result.error || 'Failed to fetch documents')
      }

      const documents = result.data.documents || result.data // Handle both paginated and non-paginated responses
      const pagination = result.data.pagination || {
        total: documents.length,
        limit: requestLimit,
        offset: requestOffset,
        hasMore: false,
      }

      const documentsCache: DocumentsCache = {
        documents,
        pagination,
        searchQuery: requestSearch,
        sortBy: requestSortBy,
        sortOrder: requestSortOrder,
        lastFetchTime: Date.now(),
      }

      set((state) => ({
        documents: {
          ...state.documents,
          [knowledgeBaseId]: documentsCache,
        },
        loadingDocuments: new Set(
          [...state.loadingDocuments].filter((loadingId) => loadingId !== knowledgeBaseId)
        ),
      }))

      logger.info(`Documents loaded for knowledge base: ${knowledgeBaseId}`)
      return documents
    } catch (error) {
      logger.error(`Error fetching documents for knowledge base ${knowledgeBaseId}:`, error)

      set((state) => ({
        loadingDocuments: new Set(
          [...state.loadingDocuments].filter((loadingId) => loadingId !== knowledgeBaseId)
        ),
      }))

      throw error
    }
  },

  getChunks: async (
    knowledgeBaseId: string,
    documentId: string,
    options?: { search?: string; limit?: number; offset?: number }
  ) => {
    const state = get()

    // Return cached chunks if they exist and match the exact search criteria AND offset
    const cached = state.chunks[documentId]
    if (
      cached &&
      cached.searchQuery === options?.search &&
      cached.pagination.offset === (options?.offset || 0) &&
      cached.pagination.limit === (options?.limit || 50)
    ) {
      return cached.chunks
    }

    // Return empty array if already loading to prevent duplicate requests
    if (state.loadingChunks.has(documentId)) {
      return cached?.chunks || []
    }

    try {
      set((state) => ({
        loadingChunks: new Set([...state.loadingChunks, documentId]),
      }))

      // Build query parameters
      const params = new URLSearchParams()
      if (options?.search) params.set('search', options.search)
      if (options?.limit) params.set('limit', options.limit.toString())
      if (options?.offset) params.set('offset', options.offset.toString())

      const response = await fetch(
        `/api/knowledge/${knowledgeBaseId}/documents/${documentId}/chunks?${params.toString()}`
      )

      if (!response.ok) {
        throw new Error(`Failed to fetch chunks: ${response.statusText}`)
      }

      const result = await response.json()

      if (!result.success) {
        throw new Error(result.error || 'Failed to fetch chunks')
      }

      const chunks = result.data
      const pagination = result.pagination

      set((state) => ({
        chunks: {
          ...state.chunks,
          [documentId]: {
            chunks, // Always replace chunks for traditional pagination
            pagination: {
              total: pagination?.total || chunks.length,
              limit: pagination?.limit || options?.limit || 50,
              offset: pagination?.offset || options?.offset || 0,
              hasMore: pagination?.hasMore || false,
            },
            searchQuery: options?.search,
            lastFetchTime: Date.now(),
          },
        },
        loadingChunks: new Set(
          [...state.loadingChunks].filter((loadingId) => loadingId !== documentId)
        ),
      }))

      logger.info(`Chunks loaded for document: ${documentId}`)
      return chunks
    } catch (error) {
      logger.error(`Error fetching chunks for document ${documentId}:`, error)

      set((state) => ({
        loadingChunks: new Set(
          [...state.loadingChunks].filter((loadingId) => loadingId !== documentId)
        ),
      }))

      throw error
    }
  },

  getKnowledgeBasesList: async (workspaceId?: string) => {
    const state = get()

    // Return cached list if we have already loaded it before (prevents infinite loops when empty)
    if (state.knowledgeBasesListLoaded) {
      return state.knowledgeBasesList
    }

    // Return cached data if already loading
    if (state.loadingKnowledgeBasesList) {
      return state.knowledgeBasesList
    }

    // Create an AbortController for request cancellation
    const abortController = new AbortController()
    const timeoutId = setTimeout(() => {
      abortController.abort()
    }, 10000) // 10 second timeout

    try {
      set({ loadingKnowledgeBasesList: true })

      const url = workspaceId ? `/api/knowledge?workspaceId=${workspaceId}` : '/api/knowledge'
      const response = await fetch(url, {
        signal: abortController.signal,
        headers: {
          'Content-Type': 'application/json',
        },
      })

      // Clear the timeout since request completed
      clearTimeout(timeoutId)

      if (!response.ok) {
        throw new Error(
          `Failed to fetch knowledge bases: ${response.status} ${response.statusText}`
        )
      }

      const result = await response.json()

      if (!result.success) {
        throw new Error(result.error || 'Failed to fetch knowledge bases')
      }

      const knowledgeBasesList = result.data || []

      set({
        knowledgeBasesList,
        loadingKnowledgeBasesList: false,
        knowledgeBasesListLoaded: true, // Mark as loaded regardless of result to prevent infinite loops
      })

      logger.info(`Knowledge bases list loaded: ${knowledgeBasesList.length} items`)
      return knowledgeBasesList
    } catch (error) {
      // Clear the timeout in case of error
      clearTimeout(timeoutId)

      logger.error('Error fetching knowledge bases list:', error)

      // Always set loading to false, even on error
      set({
        loadingKnowledgeBasesList: false,
        knowledgeBasesListLoaded: true, // Mark as loaded even on error to prevent infinite retries
      })

      // Don't throw on AbortError (timeout or cancellation)
      if (error instanceof Error && error.name === 'AbortError') {
        logger.warn('Knowledge bases list request was aborted (timeout or cancellation)')
        return state.knowledgeBasesList // Return whatever we have cached
      }

      throw error
    }
  },

  refreshDocuments: async (
    knowledgeBaseId: string,
    options?: {
      search?: string
      limit?: number
      offset?: number
      sortBy?: string
      sortOrder?: string
    }
  ) => {
    const state = get()

    // Return empty array if already loading to prevent duplicate requests
    if (state.loadingDocuments.has(knowledgeBaseId)) {
      return state.documents[knowledgeBaseId]?.documents || []
    }

    try {
      set((state) => ({
        loadingDocuments: new Set([...state.loadingDocuments, knowledgeBaseId]),
      }))

      // Build query parameters using consistent defaults
      const requestLimit = options?.limit || 50
      const requestOffset = options?.offset || 0
      const requestSearch = options?.search
      const requestSortBy = options?.sortBy
      const requestSortOrder = options?.sortOrder

      const params = new URLSearchParams()
      if (requestSearch) params.set('search', requestSearch)
      if (requestSortBy) params.set('sortBy', requestSortBy)
      if (requestSortOrder) params.set('sortOrder', requestSortOrder)
      params.set('limit', requestLimit.toString())
      params.set('offset', requestOffset.toString())

      const url = `/api/knowledge/${knowledgeBaseId}/documents${params.toString() ? `?${params.toString()}` : ''}`
      const response = await fetch(url)

      if (!response.ok) {
        throw new Error(`Failed to fetch documents: ${response.statusText}`)
      }

      const result = await response.json()

      if (!result.success) {
        throw new Error(result.error || 'Failed to fetch documents')
      }

      const documents = result.data.documents || result.data
      const pagination = result.data.pagination || {
        total: documents.length,
        limit: requestLimit,
        offset: requestOffset,
        hasMore: false,
      }

      const documentsCache: DocumentsCache = {
        documents,
        pagination,
        searchQuery: requestSearch,
        sortBy: requestSortBy,
        sortOrder: requestSortOrder,
        lastFetchTime: Date.now(),
      }

      set((state) => ({
        documents: {
          ...state.documents,
          [knowledgeBaseId]: documentsCache,
        },
        loadingDocuments: new Set(
          [...state.loadingDocuments].filter((loadingId) => loadingId !== knowledgeBaseId)
        ),
      }))

      logger.info(`Documents refreshed for knowledge base: ${knowledgeBaseId}`)
      return documents
    } catch (error) {
      logger.error(`Error refreshing documents for knowledge base ${knowledgeBaseId}:`, error)

      set((state) => ({
        loadingDocuments: new Set(
          [...state.loadingDocuments].filter((loadingId) => loadingId !== knowledgeBaseId)
        ),
      }))

      throw error
    }
  },

  refreshChunks: async (
    knowledgeBaseId: string,
    documentId: string,
    options?: { search?: string; limit?: number; offset?: number }
  ) => {
    const state = get()

    // Return cached chunks if already loading to prevent duplicate requests
    if (state.loadingChunks.has(documentId)) {
      return state.chunks[documentId]?.chunks || []
    }

    try {
      set((state) => ({
        loadingChunks: new Set([...state.loadingChunks, documentId]),
      }))

      // Build query parameters - for refresh, always start from offset 0
      const params = new URLSearchParams()
      if (options?.search) params.set('search', options.search)
      if (options?.limit) params.set('limit', options.limit.toString())
      params.set('offset', '0') // Always start fresh on refresh

      const response = await fetch(
        `/api/knowledge/${knowledgeBaseId}/documents/${documentId}/chunks?${params.toString()}`
      )

      if (!response.ok) {
        throw new Error(`Failed to fetch chunks: ${response.statusText}`)
      }

      const result = await response.json()

      if (!result.success) {
        throw new Error(result.error || 'Failed to fetch chunks')
      }

      const chunks = result.data
      const pagination = result.pagination

      set((state) => ({
        chunks: {
          ...state.chunks,
          [documentId]: {
            chunks, // Replace all chunks with fresh data
            pagination: {
              total: pagination?.total || chunks.length,
              limit: pagination?.limit || options?.limit || 50,
              offset: 0, // Reset to start
              hasMore: pagination?.hasMore || false,
            },
            searchQuery: options?.search,
            lastFetchTime: Date.now(),
          },
        },
        loadingChunks: new Set(
          [...state.loadingChunks].filter((loadingId) => loadingId !== documentId)
        ),
      }))

      logger.info(`Chunks refreshed for document: ${documentId}`)
      return chunks
    } catch (error) {
      logger.error(`Error refreshing chunks for document ${documentId}:`, error)

      set((state) => ({
        loadingChunks: new Set(
          [...state.loadingChunks].filter((loadingId) => loadingId !== documentId)
        ),
      }))

      throw error
    }
  },

  updateDocument: (knowledgeBaseId: string, documentId: string, updates: Partial<DocumentData>) => {
    set((state) => {
      const documentsCache = state.documents[knowledgeBaseId]
      if (!documentsCache) return state

      const updatedDocuments = documentsCache.documents.map((doc