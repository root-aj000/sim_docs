```typescript
/**
 * Type definitions for tokenization functionality
 */

export interface TokenEstimate {
  /** Estimated number of tokens */
  count: number
  /** Confidence level of the estimation */
  confidence: 'high' | 'medium' | 'low'
  /** Provider used for estimation */
  provider: string
  /** Method used for estimation */
  method: 'precise' | 'heuristic' | 'fallback'
}

export interface TokenUsage {
  /** Number of prompt/input tokens */
  prompt: number
  /** Number of completion/output tokens */
  completion: number
  /** Total number of tokens */
  total: number
}

export interface CostBreakdown {
  /** Input cost in USD */
  input: number
  /** Output cost in USD */
  output: number
  /** Total cost in USD */
  total: number
}

export interface StreamingCostResult {
  /** Token usage breakdown */
  tokens: TokenUsage
  /** Cost breakdown */
  cost: CostBreakdown
  /** Model used for calculation */
  model: string
  /** Provider ID */
  provider: string
  /** Estimation method used */
  method: 'tokenization' | 'provider_response'
}

export interface TokenizationInput {
  /** Primary input text */
  inputText: string
  /** Generated output text */
  outputText: string
  /** Model identifier */
  model: string
  /** Optional system prompt */
  systemPrompt?: string
  /** Optional context */
  context?: string
  /** Optional message history */
  messages?: Array<{ role: string; content: string }>
}

export interface ProviderTokenizationConfig {
  /** Average characters per token for this provider */
  avgCharsPerToken: number
  /** Confidence level for this provider's estimation */
  confidence: TokenEstimate['confidence']
  /** Supported token estimation methods */
  supportedMethods: TokenEstimate['method'][]
}
```

## Explanation of the TypeScript Code: Tokenization Type Definitions

This TypeScript file defines the data structures (interfaces) used for representing tokenization information, costs, and related configurations. Tokenization is the process of breaking down text into smaller units called "tokens," which is fundamental to how large language models (LLMs) process and understand text. These tokens are then used to estimate the cost of using an LLM.  This file acts as a central repository for all the type definitions related to tokenization, ensuring consistency and type safety across the codebase.

Here's a breakdown of each interface:

**1. `TokenEstimate`**

This interface describes an *estimate* of the number of tokens in a given text. Because the exact number of tokens can be difficult to determine without using the specific LLM's tokenizer, this interface allows for representing uncertainty in the estimate.

*   **`count: number`**:  The estimated number of tokens. This is the primary piece of information.
*   **`confidence: 'high' | 'medium' | 'low'`**:  A qualitative assessment of how reliable the token count estimate is.  `'high'` suggests a very accurate estimate, while `'low'` suggests it may be significantly off.  Using a union type here enforces that only these three confidence levels can be used.
*   **`provider: string`**: The name of the LLM provider (e.g., 'openai', 'anthropic'). This is crucial because different providers use different tokenization methods.
*   **`method: 'precise' | 'heuristic' | 'fallback'`**: Indicates the method used to estimate the tokens.
    *   `'precise'` means the estimate was likely derived from the provider's own token counter or a highly accurate library.
    *   `'heuristic'` means the estimate was based on a rule of thumb or approximation.
    *   `'fallback'` means a very simple, potentially inaccurate, method was used as a last resort.

**2. `TokenUsage`**

This interface represents the actual number of tokens used in a language model interaction. This is the *actual* token count, not an estimate.

*   **`prompt: number`**: The number of tokens used in the prompt or input provided to the language model.
*   **`completion: number`**: The number of tokens in the response or output generated by the language model.
*   **`total: number`**: The sum of `prompt` and `completion` tokens, representing the total number of tokens used for the entire interaction.

**3. `CostBreakdown`**

This interface defines the cost associated with using a language model, broken down by input and output.

*   **`input: number`**: The cost (in USD) of processing the input tokens (the `prompt`).
*   **`output: number`**: The cost (in USD) of generating the output tokens (the `completion`).
*   **`total: number`**: The total cost (in USD) of the interaction, calculated as the sum of `input` and `output`.

**4. `StreamingCostResult`**

This interface represents the cost and token usage results specifically when using a streaming API (where the output is delivered in chunks).

*   **`tokens: TokenUsage`**:  The token usage details (prompt, completion, total) for the streamed interaction.  It uses the `TokenUsage` interface defined earlier.
*   **`cost: CostBreakdown`**: The cost breakdown (input, output, total) for the streamed interaction.  It uses the `CostBreakdown` interface defined earlier.
*   **`model: string`**: The identifier of the language model used (e.g., 'gpt-3.5-turbo').
*   **`provider: string`**: The identifier of the provider (e.g., 'openai').
*   **`method: 'tokenization' | 'provider_response'`**: Indicates how the token usage was determined.
    *   `'tokenization'` means the tokens were counted by client-side tokenization logic.
    *   `'provider_response'` means the token counts were provided directly in the response from the LLM provider's API.

**5. `TokenizationInput`**

This interface defines the input data required for tokenizing text and estimating costs. This data would be sent to a function or service that performs tokenization.

*   **`inputText: string`**: The main input text that needs to be tokenized.  This is the user's query or the content to be analyzed.
*   **`outputText: string`**: The generated output text from the LLM (if available). This is necessary to calculate the `completion` token count.
*   **`model: string`**:  The name of the LLM model being used. This is critical because different models have different tokenization rules and pricing.
*   **`systemPrompt?: string`**: An optional system prompt used with the model.  System prompts influence the model's behavior and can affect token counts.  The `?` indicates that this property is optional.
*   **`context?: string`**: An optional context string.  This might be a document or other information relevant to the input text, also potentially impacting token counts. The `?` indicates that this property is optional.
*   **`messages?: Array<{ role: string; content: string }>`**: An optional array of messages representing a conversation history. Each message has a `role` (e.g., 'user', 'assistant', 'system') and `content` (the text of the message).  This is used for models that support conversational input. The `?` indicates that this property is optional.

**6. `ProviderTokenizationConfig`**

This interface defines configuration settings specific to each LLM provider that are relevant to tokenization.

*   **`avgCharsPerToken: number`**: The average number of characters per token for a given provider. This can be used as a *very rough* estimate of token count when more accurate methods are not available.
*   **`confidence: TokenEstimate['confidence']`**:  The confidence level associated with this provider's token estimation methods.  This reuses the `'high' | 'medium' | 'low'` type defined in the `TokenEstimate` interface using a type lookup.
*   **`supportedMethods: TokenEstimate['method'][]`**: An array of the token estimation methods that are supported for this provider. This uses the `'precise' | 'heuristic' | 'fallback'` type defined in the `TokenEstimate` interface using a type lookup, and makes it an array.

**Summary and Purpose**

In essence, this file provides a set of TypeScript interfaces that define the shape of data related to tokenization, cost estimation, and LLM interactions. It promotes code clarity, maintainability, and type safety by providing explicit definitions for these concepts. This is particularly important when working with LLMs, where managing token counts and costs is crucial. By using these interfaces, developers can ensure that they are handling tokenization data correctly and consistently throughout their applications.  The definitions help create more predictable, reliable, and easier-to-understand code.
